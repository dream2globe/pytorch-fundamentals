{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stretch-calculation",
   "metadata": {},
   "source": [
    "# What are Tensors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-stranger",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-component",
   "metadata": {},
   "source": [
    "- GPU accelerators를 사용할 수 있는 점을 제외하고 numpy의 ndarrays와 유사함\n",
    "- 사실, tensors와 Numpy arrays는 종종 같은 메모리를 공유하기도 함\n",
    "- 자동 미분을 최적화하는 기능이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "outer-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-communication",
   "metadata": {},
   "source": [
    "## Initializing a Tensor\n",
    "\n",
    "### Directly from data\n",
    "- 데이터 형은 자동으로 추론됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "shaped-thompson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-subdivision",
   "metadata": {},
   "source": [
    "### NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "impaired-paper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-terrace",
   "metadata": {},
   "source": [
    "### From another tensor\n",
    "- 기존 tensor의 properties(shape, data type) 정보를 활용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continuous-pregnancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "tensor([[0.3524, 0.7619],\n",
      "        [0.5994, 0.9592]])\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data)\n",
    "print(x_ones)\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "print(x_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-failing",
   "metadata": {},
   "source": [
    "### with random or constant values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "horizontal-wednesday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7674, 0.0508, 0.2153],\n",
      "        [0.4521, 0.8009, 0.5083]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(rand_tensor)\n",
    "print(ones_tensor)\n",
    "print(zeros_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-resolution",
   "metadata": {},
   "source": [
    "## Attributes of a Tensor\n",
    "- Tensor의 attribute은 shape, data type, and 저장된 장치를 표현함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "utility-notion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(tensor.shape)\n",
    "print(tensor.dtype)\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-october",
   "metadata": {},
   "source": [
    "## Operations on Tensors\n",
    "- 100개 이상의 Tensor operation은 https://pytorch.org/docs/stable/torch.html 에서 확인 가능\n",
    "- 각 operation은 GPU를 활용할 수 있으며 일반적으로 GPU의 연산이 더욱 빠름\n",
    "- Tensor는 기본적으로 CPU을 사용하도록 생성되므로 `.to` 함수를 사용하여 명시적으로 GPU로 옮겨야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "parliamentary-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-nature",
   "metadata": {},
   "source": [
    "### Standard numpy-like indexing and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "uniform-haven",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([0.5050, 0.8888, 0.3814, 0.5825])\n",
      "First column:  tensor([0.5050, 0.6953, 0.0347, 0.3831])\n",
      "Last column:  tensor([0.5825, 0.4344, 0.4942, 0.3436])\n",
      "tensor([[0.5050, 0.0000, 0.0347, 0.3831],\n",
      "        [0.8888, 0.0000, 0.9039, 0.1892],\n",
      "        [0.3814, 0.0000, 0.5761, 0.0228],\n",
      "        [0.5825, 0.0000, 0.4942, 0.3436]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(4, 4)\n",
    "print(\"First row: \", tensor[:, 0])\n",
    "print(\"First column: \", tensor[0, :])\n",
    "print(\"Last column: \", tensor[-1, :])\n",
    "\n",
    "tensor[:, 1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-desktop",
   "metadata": {},
   "source": [
    "### Joining tensors\n",
    "- torch.cat: 차원이 증가하지 않음\n",
    "- torch.stack: 차원이 증가하면서 결합됨. `unsqueeze(0)` + `cat` 효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "seventh-planning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5050, 0.0000, 0.0347, 0.3831],\n",
      "        [0.8888, 0.0000, 0.9039, 0.1892],\n",
      "        [0.3814, 0.0000, 0.5761, 0.0228],\n",
      "        [0.5825, 0.0000, 0.4942, 0.3436],\n",
      "        [0.5050, 0.0000, 0.0347, 0.3831],\n",
      "        [0.8888, 0.0000, 0.9039, 0.1892],\n",
      "        [0.3814, 0.0000, 0.5761, 0.0228],\n",
      "        [0.5825, 0.0000, 0.4942, 0.3436],\n",
      "        [0.5050, 0.0000, 0.0347, 0.3831],\n",
      "        [0.8888, 0.0000, 0.9039, 0.1892],\n",
      "        [0.3814, 0.0000, 0.5761, 0.0228],\n",
      "        [0.5825, 0.0000, 0.4942, 0.3436]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=0) # row-wide\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "palestinian-thousand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5050, 0.0000, 0.0347, 0.3831, 0.5050, 0.0000, 0.0347, 0.3831, 0.5050,\n",
      "         0.0000, 0.0347, 0.3831],\n",
      "        [0.8888, 0.0000, 0.9039, 0.1892, 0.8888, 0.0000, 0.9039, 0.1892, 0.8888,\n",
      "         0.0000, 0.9039, 0.1892],\n",
      "        [0.3814, 0.0000, 0.5761, 0.0228, 0.3814, 0.0000, 0.5761, 0.0228, 0.3814,\n",
      "         0.0000, 0.5761, 0.0228],\n",
      "        [0.5825, 0.0000, 0.4942, 0.3436, 0.5825, 0.0000, 0.4942, 0.3436, 0.5825,\n",
      "         0.0000, 0.4942, 0.3436]])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.cat([tensor, tensor, tensor], dim=1) # column-wide\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "completed-watson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5050, 0.0000, 0.0347, 0.3831],\n",
      "         [0.8888, 0.0000, 0.9039, 0.1892],\n",
      "         [0.3814, 0.0000, 0.5761, 0.0228],\n",
      "         [0.5825, 0.0000, 0.4942, 0.3436]],\n",
      "\n",
      "        [[0.5050, 0.0000, 0.0347, 0.3831],\n",
      "         [0.8888, 0.0000, 0.9039, 0.1892],\n",
      "         [0.3814, 0.0000, 0.5761, 0.0228],\n",
      "         [0.5825, 0.0000, 0.4942, 0.3436]],\n",
      "\n",
      "        [[0.5050, 0.0000, 0.0347, 0.3831],\n",
      "         [0.8888, 0.0000, 0.9039, 0.1892],\n",
      "         [0.3814, 0.0000, 0.5761, 0.0228],\n",
      "         [0.5825, 0.0000, 0.4942, 0.3436]]])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.stack([tensor, tensor, tensor], dim=0) # dim=0\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "organized-freeze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[True, True, True, True],\n",
      "         [True, True, True, True],\n",
      "         [True, True, True, True],\n",
      "         [True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True],\n",
      "         [True, True, True, True],\n",
      "         [True, True, True, True],\n",
      "         [True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True],\n",
      "         [True, True, True, True],\n",
      "         [True, True, True, True],\n",
      "         [True, True, True, True]]])\n"
     ]
    }
   ],
   "source": [
    "t4 = torch.cat([tensor.unsqueeze(0), tensor.unsqueeze(0), tensor.unsqueeze(0)], dim=0)\n",
    "print(t3 == t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "subject-cross",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5050, 0.0000, 0.0347, 0.3831],\n",
      "         [0.5050, 0.0000, 0.0347, 0.3831],\n",
      "         [0.5050, 0.0000, 0.0347, 0.3831]],\n",
      "\n",
      "        [[0.8888, 0.0000, 0.9039, 0.1892],\n",
      "         [0.8888, 0.0000, 0.9039, 0.1892],\n",
      "         [0.8888, 0.0000, 0.9039, 0.1892]],\n",
      "\n",
      "        [[0.3814, 0.0000, 0.5761, 0.0228],\n",
      "         [0.3814, 0.0000, 0.5761, 0.0228],\n",
      "         [0.3814, 0.0000, 0.5761, 0.0228]],\n",
      "\n",
      "        [[0.5825, 0.0000, 0.4942, 0.3436],\n",
      "         [0.5825, 0.0000, 0.4942, 0.3436],\n",
      "         [0.5825, 0.0000, 0.4942, 0.3436]]])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.stack([tensor, tensor, tensor], dim=1) # dim=1\n",
    "print(t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-theme",
   "metadata": {},
   "source": [
    "## Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "comparative-eligibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4030, 0.5527, 0.2213, 0.4430],\n",
       "        [0.5527, 1.6429, 0.8640, 1.0295],\n",
       "        [0.2213, 0.8640, 0.4778, 0.5147],\n",
       "        [0.4430, 1.0295, 0.5147, 0.7016]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 행렬 곱\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "micro-address",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y1 == y2) & (y1 == y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "indirect-baptist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2551, 0.0000, 0.0132, 0.2231],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0132, 0.0000, 0.3319, 0.0113],\n",
       "        [0.2231, 0.0000, 0.0113, 0.1180]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element-wise product\n",
    "z1 = tensor * tensor.T\n",
    "z2 = tensor.mul(tensor.T)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor.T, out=z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ignored-soviet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(z1 == z2) & (z1 == z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-viewer",
   "metadata": {},
   "source": [
    "### Single-element tensors\n",
    "아래 예제와 같이 모든 Tensor의 값이 하나로 합쳐지는 경우, `item()` 함수를 사용하여 Python 숫자형 값으로 젼환 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "working-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.305268287658691 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-violin",
   "metadata": {},
   "source": [
    "### In-plale operations\n",
    "- 피연산자에 결과를 저장하는 함수를 의미하며, `_` suffix로 표현됨\n",
    "- 예를 들어 `x.copy_()`, `x.t_()`는 x를 변경함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-calendar",
   "metadata": {},
   "source": [
    "> **Note:** 메모리를 절약할 수는 있지만 즉시 연산 히스토리가 사라지기 때문에 주의해서 사용 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-portfolio",
   "metadata": {},
   "source": [
    "## Bridge with NumPy\n",
    "CPU Tensor와 NumPy array는 메모리를 공유할 수 있음\n",
    " \n",
    "### Tensor to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mature-singapore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "excess-outdoors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-suspension",
   "metadata": {},
   "source": [
    "### NumPy array to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adapted-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "surprising-pierre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-cleaner",
   "metadata": {},
   "source": [
    "# Load data with PyTorch Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "demographic-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-shark",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders\n",
    "- 좀 더 읽기 쉽고 모듈화하기 위해 모델 학습코드와 데이터셋 코드가 결합지 않기를 원했음\n",
    "- Pytorch는 두 가지 데이터 기본 기능을 제공\n",
    "    - `torch.utils.data.DataLoader`\n",
    "    - `torch.utils.data.Dataset`\n",
    "- `Dataset`은 데이터와 레이블을 저장하고 `DataLoader`는 iterable 객체처럼 쉽게 읽을 수 있도록`Dataset`을 감싸는 형태\n",
    "\n",
    "### Loading a dataset\n",
    "- TorchVision을 활용하여 [Fashion-MNIST](https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/) 을 읽는 예제\n",
    "    - 60,000개의 학습 데이터와 10,000개의 테스트 데이터로 구성됨\n",
    "    - 28*28 grayscale image, 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "stainless-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # data path\n",
    "    train=True,\n",
    "    download=True, # root에 데이터가 없는 경우 다운받음\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-concrete",
   "metadata": {},
   "source": [
    "### Iterating and Visualizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "entertaining-chick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols*rows+1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-merchant",
   "metadata": {},
   "source": [
    "## Creating a Custom Dataset for your files\n",
    "- `__init__`, `__len__`, and `__getitem__` 3가지 함수를 꼭 구현해 함\n",
    "    - `__init__`: images directory, annotation file, transforms 초기화\n",
    "    - `__dataset__`: sample 수를 반환\n",
    "    - `__getitem__`: `idx`에 해당하는 이미지와 레이블을 읽어서 반환\n",
    "- FashionMNIST dataset를 직접 호출하는 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "greatest-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import torchvision.io as tvio\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotation_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotation_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = tvio.read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.transform\n",
    "        sample = {\"image\":image, \"label\":label}\n",
    "        return sample    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-surgery",
   "metadata": {},
   "source": [
    "## Preparing your data for training with DataLoaders\n",
    "\n",
    "- `Dataset`은 한 번에 하나의 샘플을 반환함\n",
    "- 모델을 학습하는 동안 minibatch\"를 사용함 (모델의 과적합을 방지하기 위해 매 epoch바다 데이터의 순서를 변경함)\n",
    "- Python의 multiprocessing을 사용하면 속도를 개선할 수 있음\n",
    "- DataLoader는 손쉬운 API를 통해 이러한 복합성을 추상화한 iterable임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "future-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "thorough-religion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Bag\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {labels_map[label.item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-cameroon",
   "metadata": {},
   "source": [
    "# Transform the data\n",
    "- 기계학습 알고리즘이 원하는 최종 처리된 형태의 데이터를 항상 얻을 수는 없기 때문에 학습에 알맞은 형태로 변환하는 경우가 많음\n",
    "- 모든 TrochVision dataset은 두 가지 Parameter(`transform`, `target_transform`)의 설정을 통해 [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)에서 제공하는 변환 함수를 사용할 수 있음 \n",
    "- FashonMNIST의 경우 PIL Image format으로 저장되어 있어 feature는 정규화된 tensor로, 레이블은 one-hot  encoded tensor의 변환이 필요함\n",
    "- 이를 위해 `ToTnesor`, `Lambda`를 사용할 예정임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "approximate-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-islam",
   "metadata": {},
   "source": [
    "### ToTensor()\n",
    "[ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor)는 PIL image, Numpy ndarray를 `FloatTensor` 로 변환하며 [0., 1.] 구간의 값으로 스케일을 변경함\n",
    "### Lambda transforms\n",
    "사용자 정의 합수 적용 시에 사용하여 위의 예시에서는 정수값을 one-hot vector로 변환하는 사용자 합수를 대입하였음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-photography",
   "metadata": {},
   "source": [
    "# Building the model layers\n",
    "## Build a neural network\n",
    "- [torch.nn](https://pytorch.org/docs/stable/nn.html) 네임스페이스는 사용자 고유의 신경망을 구성할 수 있는 모든 블럭을 제공함\n",
    "- 신경망은 다른 모듈(층)으로 구성된 그 자체이며, 중쳡된 구조는 복잡한 신경망을 쉽게 관리하고 생성할 수 있도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "composed-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-bleeding",
   "metadata": {},
   "source": [
    "### Get a hardware device for training\n",
    "GPU를 활용한 하드웨어 가속이 가능한지 확인하고 불가능하다면 CPU로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "exciting-survivor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-compression",
   "metadata": {},
   "source": [
    "### Define the class\n",
    "\n",
    "- `nn.module`을 사용해서 신경망을 정의하고 `__init__`에서 layer들을 초기화\n",
    "- 모든 `nn.module` subclass 구현은 `forward` 함수 안에서 입력 데이터에 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cutting-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-given",
   "metadata": {},
   "source": [
    "`NeuralNetwork` 인스턴스를 만들고 장치와 구조를 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "arctic-ceremony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-queensland",
   "metadata": {},
   "source": [
    "- 모델을 사용하기 위해, 입력 데이터를 통과시켜야 하며, 모델의 `forward` 함수을 실행하면 됨\n",
    "- 모델에 전달하만 백그라운드 연산에 의해 `forward` 함수가 실행되며, 직접 호출해서는 안됨\n",
    "- 모델의 호출은 각 범주의 예측값을 담은 10 차원 tensor를 반환함. 이를 nn.Softmax에 통화시기면 예측의 확률밀도를 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "incorrect-imperial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "\n",
    "\"\"\"\n",
    "tensor([[0.0983, 0.1032, 0.1035, 0.0983, 0.0983, 0.0983, 0.0983, 0.1008, 0.1027,\n",
    "         0.0983]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
    "\"\"\"\n",
    "\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-archives",
   "metadata": {},
   "source": [
    "### Model layers\n",
    "\n",
    "FashionMNIST model의 네트워크를 나누어서 살펴보고, 이미지 3개(28*28)가 포함된 미니배치를 네트워크에 통과시켜서 변화를 확인할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "minor-theorem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-powell",
   "metadata": {},
   "source": [
    "#### nn.Flatten\n",
    "[nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)는 2D 28×28 이미지를 연속된 784픽셀 이미지로 전환(미니배치 차원(at dim=0)은 유지됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adopted-synthetic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-cornwall",
   "metadata": {},
   "source": [
    "#### nn.Linear\n",
    "[linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)는 저장된 가중치와 편향을 사용하여 입력 데이터를 선형 변환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "italic-elimination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-packing",
   "metadata": {},
   "source": [
    "#### nn.ReLU\n",
    "비선형 활성 함수는 모델의 입력과 출력 사이의 복잡한 연결(비선형성)을 생성함. 이 모델에서는 [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)을 사용하였으나 다른 비성형 활성 함수를 사용할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "brilliant-enterprise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.1978, -0.2201,  0.4359,  0.5191, -0.3190, -0.2370, -0.4388,  0.6063,\n",
      "         -0.0011,  0.0814,  0.2411, -0.0049, -0.6174,  0.1177,  0.1826,  0.0585,\n",
      "         -0.1842, -0.2568,  0.0955,  0.1145],\n",
      "        [-0.0263, -0.0821,  0.1672,  0.5268, -0.1781,  0.1370, -0.5412,  0.3732,\n",
      "          0.3383,  0.1208,  0.5933, -0.2121, -0.6746,  0.0547,  0.1921, -0.2851,\n",
      "         -0.2815, -0.2852,  0.0291, -0.1473],\n",
      "        [-0.0492, -0.5013,  0.2482,  0.2134, -0.0733, -0.3063, -0.3339,  0.7264,\n",
      "          0.1527,  0.2288, -0.0534, -0.1699, -0.8194,  0.0957,  0.2727, -0.4103,\n",
      "         -0.3895, -0.3721, -0.1096,  0.1564]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.1978, 0.0000, 0.4359, 0.5191, 0.0000, 0.0000, 0.0000, 0.6063, 0.0000,\n",
      "         0.0814, 0.2411, 0.0000, 0.0000, 0.1177, 0.1826, 0.0585, 0.0000, 0.0000,\n",
      "         0.0955, 0.1145],\n",
      "        [0.0000, 0.0000, 0.1672, 0.5268, 0.0000, 0.1370, 0.0000, 0.3732, 0.3383,\n",
      "         0.1208, 0.5933, 0.0000, 0.0000, 0.0547, 0.1921, 0.0000, 0.0000, 0.0000,\n",
      "         0.0291, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2482, 0.2134, 0.0000, 0.0000, 0.0000, 0.7264, 0.1527,\n",
      "         0.2288, 0.0000, 0.0000, 0.0000, 0.0957, 0.2727, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1564]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-desktop",
   "metadata": {},
   "source": [
    "#### nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-calculation",
   "metadata": {},
   "source": [
    "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)은 모듈 들의 순서를 정의하고, 데이터는 정의한 순서대로 모든 모듈을 통과함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "gorgeous-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "\n",
    "input_image - torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-confirmation",
   "metadata": {},
   "source": [
    "#### nn.Softmax\n",
    "신경망의 마지막 층에서는 범위가 [-infty, infty] 인 `logits`을 반환하므로 [nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)을 사용하여 [0, 1] 사이의 값 즉, 각 범주의 예측 확률변수로 나타낼 수 있음. `dim` parameter는 해당 축의 합이 1인 차원을 가리킴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "welsh-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-postcard",
   "metadata": {},
   "source": [
    "### Model parameters\n",
    "\n",
    "신경망의 많은 layer는 파라미터로 나타낼 수 있으며, 파라미터 학습 중 최적화됨. `nn.Module` 를 상속하면 자동으로 모델 내 파라미터들이 추적되며, 모든 파라미터는 모델의 `parameters()` 혹은 `named_parameters()` 함수로 접근할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "associate-warrant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : Parameter containing:\n",
      "tensor([[-0.0086,  0.0239, -0.0357,  ...,  0.0343,  0.0206, -0.0123],\n",
      "        [ 0.0189, -0.0171,  0.0202,  ...,  0.0341,  0.0136,  0.0257],\n",
      "        [-0.0102, -0.0332,  0.0130,  ...,  0.0234, -0.0336, -0.0092],\n",
      "        ...,\n",
      "        [-0.0231, -0.0305,  0.0154,  ..., -0.0051,  0.0187, -0.0177],\n",
      "        [ 0.0202, -0.0222, -0.0080,  ...,  0.0005, -0.0284, -0.0005],\n",
      "        [ 0.0066, -0.0343,  0.0037,  ...,  0.0334, -0.0224, -0.0167]],\n",
      "       device='cuda:0', requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : Parameter containing:\n",
      "tensor([ 3.3450e-02, -2.6417e-04,  1.3387e-02,  2.0698e-02,  2.3131e-02,\n",
      "        -1.4283e-02, -1.2606e-04, -2.3919e-02,  2.7788e-02, -9.7004e-03,\n",
      "        -2.7476e-02,  2.6992e-02, -1.2105e-02, -1.8845e-02,  2.3516e-03,\n",
      "        -2.8786e-02,  1.3932e-02,  2.1549e-02, -1.9627e-02,  1.3212e-02,\n",
      "         3.4026e-02, -2.3801e-02, -1.6099e-02, -3.5709e-02,  1.1356e-02,\n",
      "         2.3780e-02, -2.9032e-02, -7.2641e-03,  2.3025e-02,  3.1137e-02,\n",
      "         2.0344e-02,  3.3691e-02, -1.5006e-02,  3.4777e-02, -4.8937e-03,\n",
      "        -1.0049e-02,  3.4435e-03, -6.7316e-03,  1.0959e-02, -2.9270e-02,\n",
      "         2.1023e-02,  2.1289e-02,  3.1833e-02, -2.6880e-02, -2.2705e-02,\n",
      "         1.7342e-02, -3.0246e-03, -9.5787e-03, -2.7710e-02, -1.1558e-02,\n",
      "         1.8155e-03, -3.5543e-02, -2.3787e-02,  2.7108e-02, -3.1292e-02,\n",
      "         9.0474e-03,  9.2301e-03, -7.2627e-03,  1.3545e-02,  2.7695e-02,\n",
      "        -2.1896e-02,  1.6448e-02,  3.1803e-03,  2.6769e-02, -2.3839e-02,\n",
      "         1.6977e-02, -1.1683e-02, -1.3331e-02, -8.4130e-03,  1.7245e-02,\n",
      "        -1.6297e-02, -2.6593e-02, -1.7095e-02,  3.0812e-02, -5.6151e-03,\n",
      "        -1.9578e-02, -2.9273e-02, -1.0683e-02, -4.8191e-03, -2.1834e-02,\n",
      "        -2.0186e-02, -3.5233e-02,  4.3348e-03, -2.2555e-03, -1.4806e-02,\n",
      "        -4.2670e-03, -6.2506e-03, -1.4848e-02,  3.1982e-02, -9.0463e-03,\n",
      "        -1.2431e-02, -2.2460e-02,  1.7020e-02,  1.9442e-02,  1.7571e-02,\n",
      "        -1.3058e-02, -3.5347e-02, -3.0018e-02, -1.3695e-02, -3.1178e-03,\n",
      "         2.0849e-03, -2.3648e-02, -2.7899e-02,  1.4271e-02, -1.7336e-02,\n",
      "         9.5740e-03, -5.9981e-03, -2.1116e-02, -2.1687e-02,  1.7545e-02,\n",
      "        -1.6662e-02, -1.7736e-03, -3.3737e-02, -6.1413e-03, -2.0046e-02,\n",
      "        -1.1903e-02,  1.8996e-02, -2.4583e-02,  3.2182e-02,  2.0608e-03,\n",
      "        -1.3136e-02, -2.1836e-02,  3.2528e-03,  8.8970e-03, -2.9623e-02,\n",
      "         9.8799e-03, -3.1430e-02,  1.3339e-02,  1.4376e-03, -3.0532e-03,\n",
      "        -1.9481e-02,  1.1368e-02, -1.1824e-02,  3.1022e-02,  6.7259e-03,\n",
      "        -2.3383e-02,  3.5532e-02, -8.9601e-03,  7.3844e-03, -3.0995e-02,\n",
      "        -2.3195e-02, -2.2290e-02, -2.3569e-02,  1.9015e-03,  1.4105e-02,\n",
      "         3.0401e-02,  2.8199e-02,  1.6260e-02,  2.6880e-02,  1.6965e-03,\n",
      "         2.6575e-02, -1.6980e-03, -9.9148e-03,  5.5117e-03,  2.5153e-02,\n",
      "         2.3647e-02, -2.7235e-02,  1.6736e-02,  2.7831e-02,  3.5735e-03,\n",
      "         2.3971e-02,  1.5119e-02, -1.4202e-02,  1.5380e-02,  2.0997e-02,\n",
      "         7.3782e-03,  3.3872e-02, -7.7986e-03, -1.2796e-02, -2.5438e-02,\n",
      "         1.5670e-02, -3.9551e-03,  7.1171e-03, -1.9704e-02,  4.3810e-03,\n",
      "         1.7435e-02, -4.3940e-03, -1.8290e-02,  5.3176e-03, -1.4473e-02,\n",
      "        -3.0169e-02,  1.7647e-02, -3.1156e-02, -1.5762e-02, -2.4933e-02,\n",
      "         2.1387e-02, -2.7293e-02, -3.2057e-02, -2.6355e-02, -9.4544e-04,\n",
      "        -2.6246e-02, -2.9958e-03,  3.3317e-02,  8.1439e-03,  1.7668e-02,\n",
      "         1.9252e-02, -2.9016e-02,  3.0782e-02, -7.0007e-03,  7.0395e-03,\n",
      "        -2.3142e-02, -2.4224e-02,  3.3727e-02, -6.5615e-04, -2.8141e-02,\n",
      "         6.4455e-03,  1.5806e-02,  3.1299e-02, -1.1587e-02,  5.7410e-03,\n",
      "        -3.6184e-03,  2.1863e-02, -2.4179e-02, -1.6430e-02, -1.8335e-02,\n",
      "        -1.4973e-02, -1.8357e-02,  1.3350e-02,  2.7274e-02,  1.3287e-02,\n",
      "        -2.9833e-02, -1.4466e-03,  1.2375e-02, -2.3393e-02, -1.4308e-02,\n",
      "        -1.1041e-02,  3.4830e-02,  1.2719e-02,  2.6381e-02, -3.3077e-02,\n",
      "         2.1475e-03,  2.3538e-02, -2.7391e-02, -1.7331e-02, -1.5460e-02,\n",
      "         2.6299e-02,  5.3507e-03,  2.1975e-02, -2.2322e-02, -2.9588e-02,\n",
      "         1.4894e-02, -2.0685e-02, -1.2163e-02,  3.4022e-02, -3.5166e-02,\n",
      "        -1.6236e-02, -2.1046e-02,  2.5619e-02,  1.7441e-02,  2.2467e-02,\n",
      "         8.9422e-03, -3.4990e-02,  6.8197e-03, -3.1609e-02,  1.3032e-02,\n",
      "         1.4402e-02, -1.9799e-03, -2.5154e-02,  2.5353e-02, -3.1827e-02,\n",
      "         2.5645e-02,  2.5505e-02, -3.3160e-02, -7.3864e-03, -2.7523e-02,\n",
      "        -2.8603e-02, -3.0323e-02,  1.2620e-02, -3.4446e-02,  2.9270e-02,\n",
      "        -1.7835e-02, -2.4046e-02,  1.1144e-03, -2.9037e-03, -2.7056e-02,\n",
      "         2.8319e-03, -2.3540e-02, -3.3039e-02,  3.2470e-02, -9.5189e-03,\n",
      "         3.2797e-03, -3.0062e-03,  2.6491e-02, -3.4851e-02,  3.3665e-02,\n",
      "         1.0266e-02,  2.6418e-03,  7.1979e-03, -2.6473e-02, -1.8284e-02,\n",
      "        -3.5171e-02, -3.5052e-02,  1.3508e-02, -1.3721e-02,  5.8740e-03,\n",
      "        -7.5198e-03,  2.1642e-02, -3.2172e-02, -3.5041e-02,  1.8765e-02,\n",
      "         2.7852e-03,  2.9551e-03, -2.1577e-02,  2.5695e-02, -2.9695e-02,\n",
      "        -2.5441e-02, -2.6529e-02, -5.2514e-03,  1.0703e-03, -1.5563e-02,\n",
      "         9.1106e-03, -2.4219e-03,  2.9247e-02,  9.4658e-03, -1.3538e-02,\n",
      "        -2.9481e-02, -1.0056e-02, -2.3736e-02,  1.0758e-02, -8.5132e-03,\n",
      "         2.6992e-02, -1.7622e-02, -7.8529e-03, -3.2940e-02, -3.6880e-04,\n",
      "        -3.5384e-02, -6.0193e-03, -7.5025e-03, -2.3820e-02,  1.0103e-02,\n",
      "        -1.9892e-02, -1.5512e-02,  2.0574e-02,  5.8576e-03, -3.4685e-02,\n",
      "         1.3568e-02,  3.5613e-02,  1.6858e-02,  1.7085e-02,  3.4969e-02,\n",
      "        -2.8843e-02,  2.8062e-02,  3.0777e-02, -3.3107e-03, -3.0747e-02,\n",
      "        -4.5006e-03,  2.1475e-02,  2.0187e-02,  1.1345e-02,  1.1779e-02,\n",
      "        -2.3664e-03,  1.1382e-02,  2.3239e-02, -2.9600e-02, -1.3154e-02,\n",
      "        -1.6670e-02, -2.6661e-03,  1.0556e-02,  9.1428e-03,  1.3317e-02,\n",
      "        -6.9359e-03,  7.5668e-03,  3.1041e-02, -2.2082e-02, -1.9109e-02,\n",
      "         9.0284e-05, -3.0626e-02, -1.4300e-02,  3.1651e-02,  2.7185e-02,\n",
      "         2.7420e-02, -7.0258e-03,  2.2393e-02, -1.3853e-02,  3.0868e-02,\n",
      "        -7.9145e-03,  4.6259e-03,  4.3358e-03, -1.0433e-02,  2.5614e-02,\n",
      "        -2.9970e-02,  2.5668e-02, -8.7792e-03, -6.3040e-03,  2.3529e-02,\n",
      "         2.5138e-02,  1.3641e-02, -2.9720e-02, -1.2513e-02, -4.7026e-03,\n",
      "        -1.0464e-02,  1.1763e-02, -1.1231e-02,  4.2963e-03, -3.2966e-03,\n",
      "         1.3005e-02,  7.7143e-03, -2.7055e-02,  3.3876e-02, -8.4210e-03,\n",
      "        -3.1690e-02, -3.2063e-03, -1.3024e-02,  1.6599e-02,  2.3851e-02,\n",
      "         8.5312e-03,  3.2728e-02,  3.9340e-03, -2.7757e-02, -3.4285e-02,\n",
      "        -1.3587e-02,  1.9889e-02,  2.1110e-02,  1.9576e-02, -7.9917e-03,\n",
      "        -1.8727e-02, -1.1793e-02, -2.1380e-02, -4.5913e-04, -5.9209e-04,\n",
      "         1.1220e-02, -8.7964e-03,  3.0190e-02, -1.4606e-02,  1.3408e-02,\n",
      "        -2.9139e-02,  2.5144e-02,  2.8512e-02, -3.4299e-02, -1.0418e-02,\n",
      "         3.5512e-02,  3.3708e-03,  2.6774e-02, -1.2734e-02,  2.2061e-03,\n",
      "         7.9702e-03, -2.2480e-02,  2.7210e-03, -8.0455e-03, -3.0791e-02,\n",
      "        -2.6360e-02,  2.4089e-02,  4.2736e-03, -1.1531e-03, -2.1160e-02,\n",
      "        -3.5473e-02, -8.6912e-03, -1.7274e-02, -1.7631e-02, -2.4564e-02,\n",
      "        -1.6978e-02,  1.5038e-02,  1.1206e-03, -2.8958e-02,  2.1150e-02,\n",
      "        -1.2803e-02, -4.4420e-03,  3.4592e-02,  3.1364e-02, -3.3761e-02,\n",
      "        -6.5875e-03,  2.7099e-02,  3.4020e-02,  3.5091e-02, -1.7205e-02,\n",
      "         1.3545e-02,  8.6785e-03,  3.0017e-02,  1.4973e-02, -1.6356e-02,\n",
      "         1.2314e-02, -1.4842e-02,  3.2739e-02,  7.6589e-03, -8.3638e-05,\n",
      "        -3.3898e-02,  2.2961e-02, -3.4134e-02,  1.8209e-02,  2.1670e-02,\n",
      "         1.5883e-03, -2.8329e-02,  3.3715e-02,  2.4574e-02, -2.5856e-02,\n",
      "        -8.5947e-03,  2.8143e-02,  2.7837e-02,  1.1368e-02,  2.4706e-02,\n",
      "         2.2749e-02, -2.1729e-03, -5.1960e-03, -1.5336e-02,  3.3815e-02,\n",
      "         1.5044e-02,  1.4243e-03, -2.0257e-02,  2.9531e-02, -1.8623e-02,\n",
      "        -2.4313e-03, -3.0231e-02, -2.6457e-03, -2.7461e-02, -7.9592e-03,\n",
      "         3.0093e-02, -3.2427e-02, -1.4879e-02,  9.6872e-03, -1.5604e-02,\n",
      "        -5.6308e-03, -2.1632e-02], device='cuda:0', requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : Parameter containing:\n",
      "tensor([[ 0.0151, -0.0335, -0.0321,  ...,  0.0312, -0.0269,  0.0323],\n",
      "        [ 0.0205,  0.0080,  0.0094,  ..., -0.0428, -0.0344,  0.0288],\n",
      "        [-0.0107, -0.0328, -0.0073,  ...,  0.0132, -0.0234, -0.0271],\n",
      "        ...,\n",
      "        [ 0.0104, -0.0041,  0.0435,  ..., -0.0325,  0.0272,  0.0068],\n",
      "        [ 0.0058, -0.0180,  0.0184,  ..., -0.0199,  0.0209,  0.0350],\n",
      "        [ 0.0200, -0.0342, -0.0328,  ..., -0.0307,  0.0117, -0.0182]],\n",
      "       device='cuda:0', requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : Parameter containing:\n",
      "tensor([-3.8291e-04,  1.4790e-03,  1.9151e-02,  4.3658e-02, -2.3544e-02,\n",
      "        -3.3796e-02, -4.4162e-02,  1.0804e-02, -3.4815e-02, -2.8033e-02,\n",
      "         2.0028e-02, -2.0086e-02,  1.0346e-02,  8.9639e-03,  3.7176e-02,\n",
      "        -5.5904e-03, -1.2816e-02, -2.4724e-02, -4.0995e-02, -3.3443e-02,\n",
      "        -6.8654e-03, -4.3784e-02,  8.7296e-03, -2.4178e-02,  7.7561e-03,\n",
      "         4.4231e-03, -2.6237e-02,  2.0159e-03,  4.1591e-02, -2.0748e-03,\n",
      "        -2.1647e-02, -3.3561e-02, -2.8966e-02, -2.9602e-02,  1.3453e-02,\n",
      "        -2.1414e-03,  2.9749e-02,  2.1857e-02, -9.8336e-04, -6.4977e-03,\n",
      "        -6.9464e-03,  3.8672e-02, -1.3548e-02, -3.7695e-02, -3.5624e-02,\n",
      "         3.5847e-02, -2.9810e-02,  5.6666e-05, -2.6092e-02, -2.7394e-02,\n",
      "         3.2854e-02, -1.2456e-02,  2.6198e-02,  1.0706e-02, -1.8401e-02,\n",
      "        -3.8384e-03,  2.6932e-02, -3.1861e-02, -1.8618e-02,  3.1185e-02,\n",
      "         2.0988e-03, -3.4388e-02,  1.5816e-02,  9.9030e-03,  1.2010e-02,\n",
      "        -3.0934e-02, -1.3394e-02,  9.2301e-03,  1.0399e-02,  6.3388e-03,\n",
      "         1.6414e-02,  2.7820e-02,  4.3269e-02, -2.3703e-02,  2.4569e-02,\n",
      "        -3.7333e-02,  1.4561e-03,  9.4761e-03, -3.0559e-03,  1.2306e-02,\n",
      "        -4.7613e-03, -3.4452e-02,  3.2929e-02,  4.2632e-02,  4.0900e-02,\n",
      "        -1.1449e-03,  1.3023e-02, -3.5896e-02,  3.7532e-02,  4.0720e-02,\n",
      "        -1.8452e-02,  3.3989e-02,  1.2801e-03,  3.0050e-02, -1.9689e-02,\n",
      "        -1.6062e-02,  3.5088e-02, -5.0626e-03, -3.3734e-02, -3.7266e-02,\n",
      "         3.1310e-02, -2.3489e-02,  1.9462e-02,  2.5898e-03, -2.6488e-02,\n",
      "         2.8968e-02,  1.1401e-02,  3.8114e-02, -3.2183e-02, -3.9736e-02,\n",
      "         2.1831e-02, -2.0064e-02, -6.3448e-03,  3.8576e-03, -1.7756e-02,\n",
      "        -3.1084e-02, -1.6291e-02, -3.7466e-02, -1.4963e-03,  3.8732e-03,\n",
      "         2.0834e-03, -3.2002e-02,  4.4048e-02,  5.9588e-03, -1.2737e-03,\n",
      "        -1.5114e-02,  3.9972e-02, -3.2875e-02, -3.5358e-02,  3.3720e-02,\n",
      "         3.6367e-02, -4.1926e-03, -3.9798e-02, -1.1938e-02,  6.6694e-03,\n",
      "         4.0549e-03, -3.5493e-02,  4.5026e-03,  4.1054e-02, -1.6417e-03,\n",
      "        -6.0120e-03, -9.4493e-04,  2.5456e-02, -2.6692e-02,  1.2008e-02,\n",
      "         1.3701e-02, -2.1338e-02, -4.2986e-02, -3.1200e-02,  2.5827e-02,\n",
      "         1.7177e-02, -7.4683e-03,  2.2661e-02,  1.1121e-02,  1.1387e-02,\n",
      "         3.3707e-02, -2.3539e-03,  8.7443e-03, -2.5690e-02, -4.0692e-02,\n",
      "        -1.5027e-02,  3.3429e-02, -4.4937e-03, -2.3648e-02, -3.4478e-02,\n",
      "         1.8510e-02,  2.1397e-03,  3.2865e-02,  1.9635e-02,  1.1056e-02,\n",
      "        -9.0261e-03,  4.3700e-02, -3.4142e-02,  2.2519e-04, -1.2014e-03,\n",
      "        -1.5266e-02,  4.2246e-02, -4.0553e-02,  1.8451e-02,  9.3294e-03,\n",
      "        -3.8305e-02, -1.6713e-02,  3.4555e-02, -4.3583e-02, -3.2650e-03,\n",
      "        -1.9449e-02,  2.9432e-02, -1.7172e-02,  8.1133e-03,  1.6899e-02,\n",
      "        -3.4319e-02,  4.1902e-02, -1.4684e-02, -5.6577e-03, -4.2524e-02,\n",
      "        -2.5616e-02, -4.2939e-02,  5.3506e-03, -2.9942e-02,  1.5027e-02,\n",
      "        -9.1662e-03,  3.0930e-03, -3.0982e-02, -2.3209e-02,  2.0913e-02,\n",
      "        -2.5090e-02, -1.3276e-02,  2.1314e-02, -2.4733e-02,  1.5039e-03,\n",
      "        -1.6698e-02, -1.1883e-02,  1.0353e-03, -3.3527e-03,  4.0492e-02,\n",
      "         3.7974e-02,  5.7153e-03,  3.7660e-02,  3.7942e-02,  1.4942e-03,\n",
      "        -4.1226e-03,  1.9205e-02, -1.2878e-03, -3.6683e-02, -3.8326e-02,\n",
      "        -3.6530e-02,  3.3195e-02, -1.7725e-02,  4.0818e-02, -1.6936e-02,\n",
      "        -5.1971e-03,  1.7774e-02, -4.3509e-02,  3.0710e-02,  4.2528e-02,\n",
      "        -5.2610e-03,  1.2788e-02,  2.3028e-02,  2.5087e-02,  1.8064e-02,\n",
      "         3.7381e-02,  2.7086e-02, -2.1580e-03, -4.2152e-02, -1.0245e-02,\n",
      "         2.5493e-02,  3.6075e-02,  1.8016e-02, -3.8537e-02,  5.0007e-03,\n",
      "        -3.0112e-02,  1.6310e-02, -4.1620e-02, -3.4794e-03, -4.1204e-02,\n",
      "        -7.6438e-03, -5.9420e-03, -2.1737e-02,  1.2360e-02, -3.3816e-03,\n",
      "        -3.1031e-02, -2.3725e-02, -7.7366e-03, -4.0567e-02, -1.3021e-02,\n",
      "         2.3175e-02, -2.0402e-03, -2.0979e-02,  1.1494e-02,  1.5665e-02,\n",
      "         3.5610e-02,  4.2361e-02, -2.8619e-02, -3.8797e-02, -2.3282e-03,\n",
      "         6.7949e-03, -4.3689e-02,  2.3721e-02,  2.0653e-02, -1.4870e-02,\n",
      "         2.6906e-02,  2.7882e-02,  5.9949e-03,  1.2475e-02, -4.2439e-02,\n",
      "        -9.6057e-03,  1.0525e-02, -2.8297e-02,  1.6908e-02,  3.5581e-02,\n",
      "        -1.3109e-02,  3.6388e-02, -4.2609e-02, -3.3186e-02,  7.6671e-03,\n",
      "        -2.9174e-02, -2.9450e-03, -3.2707e-02,  1.8873e-02, -3.6676e-03,\n",
      "        -7.9934e-03,  2.1023e-02, -8.9308e-03,  8.8881e-03, -3.7109e-02,\n",
      "        -4.0906e-02, -1.8360e-02, -2.9228e-02,  4.1591e-02, -9.9184e-04,\n",
      "        -3.7555e-02,  3.9052e-02,  1.8519e-02,  4.3757e-02, -3.4387e-02,\n",
      "         2.0648e-02, -2.5667e-03,  2.5025e-02, -1.5075e-02, -5.1361e-03,\n",
      "         2.1225e-02, -1.2467e-03, -4.7931e-03, -2.4389e-02, -1.4393e-02,\n",
      "         3.9054e-02,  1.0411e-02, -3.4887e-02, -4.1979e-02, -3.6128e-02,\n",
      "         2.4988e-02,  1.7700e-02, -4.3264e-02, -2.1490e-02,  5.4050e-03,\n",
      "         2.4247e-02,  3.2901e-02,  5.1887e-03, -3.7263e-02, -1.6552e-02,\n",
      "        -1.5747e-02,  1.4193e-03,  6.8109e-03, -2.9416e-02,  5.5967e-03,\n",
      "         1.2564e-02, -1.7290e-02,  3.1299e-02, -1.2050e-02, -1.2529e-02,\n",
      "        -1.7730e-02, -3.0071e-02, -3.6722e-02, -2.3205e-02,  1.5899e-02,\n",
      "         3.6721e-02, -1.7631e-02, -2.5851e-02,  2.1569e-02, -2.5587e-02,\n",
      "         2.1371e-02,  6.1490e-03, -2.9686e-02,  3.2970e-03,  1.6666e-02,\n",
      "        -4.2812e-03, -6.0399e-03,  1.0818e-02, -3.3853e-02, -3.8999e-02,\n",
      "        -1.5375e-02,  2.0684e-02, -6.9726e-03,  1.6266e-02,  3.5750e-02,\n",
      "        -1.5704e-02,  3.3698e-02, -3.5332e-02, -2.2875e-02, -2.5456e-02,\n",
      "         2.6108e-02, -4.0795e-02,  2.1041e-02,  3.5615e-02,  4.2886e-02,\n",
      "        -4.2458e-02,  5.3575e-03, -2.9226e-02, -3.5000e-02, -2.7764e-02,\n",
      "        -2.6089e-02, -2.4650e-02,  2.9157e-03,  4.3721e-02, -2.4348e-02,\n",
      "         4.0670e-02,  4.0408e-02, -7.2405e-03,  1.2641e-02,  2.2173e-02,\n",
      "         1.7295e-02,  2.6820e-02,  1.1353e-02,  2.9515e-02,  3.9588e-02,\n",
      "        -2.0440e-02,  2.6896e-03,  8.1800e-03,  1.6499e-02,  9.6089e-03,\n",
      "         2.1719e-02, -2.2179e-02,  1.0777e-02,  2.7018e-02, -2.9236e-02,\n",
      "        -7.0239e-03, -2.1124e-02, -3.9295e-02, -2.4559e-02,  7.7847e-03,\n",
      "        -3.5042e-02,  6.7424e-03, -1.3314e-02, -2.5290e-02,  4.1915e-02,\n",
      "         3.0446e-02,  2.0210e-02, -2.0862e-02,  3.2693e-02, -1.4370e-02,\n",
      "         2.4756e-02, -4.3896e-02, -3.5085e-02,  3.0009e-02, -6.5586e-03,\n",
      "        -4.1040e-02, -1.6755e-04,  2.2681e-02, -3.7478e-03,  1.3245e-02,\n",
      "         4.0091e-02,  1.1729e-02, -5.5369e-03, -3.8841e-03,  3.3514e-02,\n",
      "        -5.1790e-03,  1.3066e-02, -2.9022e-02,  2.5492e-02,  2.3847e-02,\n",
      "         3.9701e-02,  1.7460e-02,  4.9294e-03, -2.2738e-02, -1.2706e-02,\n",
      "        -3.0960e-02,  2.8959e-02, -1.2119e-02, -3.7188e-02,  1.8525e-02,\n",
      "        -2.7709e-02,  1.7817e-02,  1.0849e-02,  1.8764e-03,  2.6901e-02,\n",
      "        -1.2424e-02,  3.7171e-02, -6.1610e-03, -2.9366e-02,  3.4034e-02,\n",
      "         1.0214e-02,  2.4368e-02,  1.7943e-02,  5.4437e-03, -2.7376e-02,\n",
      "        -1.3432e-02,  9.9126e-03,  1.9129e-02, -3.5707e-02,  3.8423e-02,\n",
      "         4.6347e-03, -3.9158e-02,  2.0934e-03, -3.2999e-02,  1.2701e-02,\n",
      "         1.9977e-02,  2.9579e-02,  1.4422e-02,  3.2300e-02,  2.2023e-03,\n",
      "         7.1191e-03,  1.1050e-03, -1.5496e-02, -4.2300e-02,  1.9820e-02,\n",
      "        -5.8243e-04,  3.1620e-03,  2.6340e-02, -1.0929e-02,  2.4865e-02,\n",
      "        -9.3756e-04, -3.4496e-02, -9.4876e-03, -2.0399e-02, -1.7327e-02,\n",
      "         3.7164e-02,  9.8631e-03, -1.7769e-02, -1.6813e-02, -5.8282e-03,\n",
      "         3.9325e-02,  3.0474e-02], device='cuda:0', requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : Parameter containing:\n",
      "tensor([[-0.0252,  0.0014, -0.0127,  ..., -0.0278, -0.0433, -0.0219],\n",
      "        [ 0.0177,  0.0197,  0.0143,  ..., -0.0308, -0.0046,  0.0024],\n",
      "        [ 0.0107, -0.0109, -0.0100,  ...,  0.0064, -0.0188, -0.0342],\n",
      "        ...,\n",
      "        [-0.0368,  0.0009, -0.0003,  ...,  0.0212,  0.0398, -0.0320],\n",
      "        [-0.0020, -0.0408, -0.0301,  ..., -0.0326,  0.0160,  0.0180],\n",
      "        [-0.0026,  0.0201,  0.0137,  ...,  0.0422,  0.0033,  0.0175]],\n",
      "       device='cuda:0', requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : Parameter containing:\n",
      "tensor([ 0.0266,  0.0393,  0.0097, -0.0003,  0.0081, -0.0048, -0.0268,  0.0171,\n",
      "         0.0261, -0.0135], device='cuda:0', requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-label",
   "metadata": {},
   "source": [
    "# Automatic differentialtion\n",
    "## Automatic differentiation with torch.autograd\n",
    "- 신경망을 학습할 때 **back propagation** 을 주로 사용하며, 손실함수의 **gradient**에 따라 모델의 weigth가 조절됨\n",
    "- Pytorch에는 `torch.autograd` 라는 내장 미분계산 엔진이 있어 gradient 자동 계산을 지원함\n",
    "- 입력 백터 `x`, 파라미터 `w`, `b` 그리고 어떤 손실함수로 구성된 단일층 신경망을 고려했을 때, 아래와 같이 Pytorch로 정의할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "sudden-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-refrigerator",
   "metadata": {},
   "source": [
    "## Tensors, Functions and Computational graph\n",
    "- 위 코드는 아래의 **computational graph**를 정의함\n",
    "![Diagram showing a computational graph with two parameters 'w' and 'b' to compute the gradients of loss.](images/computational-graph.png)\n",
    "- `w`, `b`는 최적화가 필요한 파라미터이므로 손실함수의 gradients의 자동 계산을 위해 `requires_grad` 를 true로 설정하였음\n",
    "> Tensor를 생성할 때 requires_grad를 설정하거나 만든 뒤에는 x.requires_grad_(True)로 설정 가능함\n",
    "- computational graph 생성을 위해 실행한 함수는 사실 `Funtion` class의 object임\n",
    "- 이 object는 forword 방향의 연산과, 미분하는 동안의 backward propagarion의 계산을 어떻게 해야하는지 알고 있으며, Tensor의 `grad_fn` property에 저장하고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "legal-november",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7f5660416f90>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x7f563cb31310>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =',z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-machinery",
   "metadata": {},
   "source": [
    "미분 계산을 위해 `loss.backward()`을 호출 후 `w.grad`, `b.grad`를 호출하여 값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ordered-ordering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2564, 0.0207, 0.3236],\n",
      "        [0.2564, 0.0207, 0.3236],\n",
      "        [0.2564, 0.0207, 0.3236],\n",
      "        [0.2564, 0.0207, 0.3236],\n",
      "        [0.2564, 0.0207, 0.3236]])\n",
      "tensor([0.2564, 0.0207, 0.3236])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-nursery",
   "metadata": {},
   "source": [
    "> **Note** requires_grad 설정이 True인 leaf node에서만 grad를 사용할 수 있음. 또한 성능을 고려하여 주어진 그래프에서 gradient는 한번만 호출될 수 있음. 같은 그래프에서 반복적으로 호출하기 위해서는 `backward` 호출에 retain_graph=True로 설정해야 함\n",
    "\n",
    "### Disabling gradient tarcking\n",
    "모든 Tensor는 기본적으로 `requires_trad=True`로 설정되어 계산 이력이 추적되어, gradient 계산을 지원함. 그러나 학습된 모델에서 입력값을 추론하는 경우에는 단지 forward만 필요할 것임. 이러한 경우 `with torch.no_grad()` 블럭을 사용하여 기본 동작을 정지할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "warming-prior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-merchant",
   "metadata": {},
   "source": [
    "`detach` 함수를 사용하여 같은 결과를 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "original-replacement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-baltimore",
   "metadata": {},
   "source": [
    "gradient tracking을 정지하는 이유는 아래와 같음\n",
    "- [fine tuning a pre-trained network](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html) 에서 자주 사용하는 시나리오로써 신경망의 파라미터를 고정하기 위함\n",
    "- 이미 학습이 끝난 모델을 사용하여 추론하는 경우 forward pass만 필요하므로 gradient 추적을 하지 않음으로써 성능 향상이 가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-preference",
   "metadata": {},
   "source": [
    "## More on Computational Graph\n",
    "\n",
    "개념적으로 Autograd는 Function Object로 구성된 DAG 안에 있는 모든 연산자와 데이터를 유지함. DAG에서 leaf는 입력 Tensor를 root는 출력 Tensor이며, Leaf(input)에서 Root(output)까지 추적이 가능하므로, chain rule을 사용하여 자동으로 gradient를 계산할 수 있음.\n",
    "\n",
    "Forward pass에서 autograd는 두 가지를 동시에 수행함\n",
    "- 연산자를 실행하여 Tensor의 계산 결과를 제공함\n",
    "- DAG에서 연산자의 gradient 기능을 유지\n",
    "\n",
    "Backward pass에서 DAG root에서 `backward()`가 호출되었을 때 autograd는\n",
    "- 각 `.grad_fn`에서 gradient를 계산\n",
    "- Chain Rule를 사용하여 각 Tensor의 .grad 속성을 누적 계산하고, leaf tensor까지 모든 방향으로 전파함\n",
    "\n",
    "### DAGs are dynamic in Pytorch\n",
    "- 주목할 점은 그래프가 처음부터 다시 생성된다는 것\n",
    "- `.backward()` 호출 후 autograd는 새 그래프를 생성하기 시작. 이것이 바로 모델에서 제어 흐름을 사용할 수 있도록 함. 필요한 경우 반복할 때 마다 모양, 크기 및 연산의 변경이 가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-russian",
   "metadata": {},
   "source": [
    "# Learn about the optimization loop\n",
    "## Optimizing the model parameters\n",
    "- 모델을 학습하는 것은 반복적인 프로세스이며 epoch로 불리우는 각 반복 단계에서 모델은 출력과 에러를 계산\n",
    "- 이후에는 모듈에서의 파라미터와 관련된 에러의 미분값을 수집하고 경사하강법을 사용하여 파라미터를 최적화함\n",
    "- 이 프로세스의 자세한 내용은 [backpropagation from 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)를 참고바람\n",
    "\n",
    "### Prerequisite code\n",
    "앞에서 소개된 코드로부터 **Dataset**, **DataLoaders**, **Build Model** 모듈이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fabulous-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "executive-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-attack",
   "metadata": {},
   "source": [
    "## Setting hyperparameters\n",
    "하이퍼파라미터는 모델 최적화 프로세스를 제어하는 조절 가능한 파라미터를 의미함. 하이퍼파라미터는 모델 학습과 수렴 속도에 영향을 줌([참고](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html))\n",
    "\n",
    "모델 훈련을 위해 다음의 하이퍼파라미터를 정의\n",
    "- **Number of Epochs** - 데이터 전체의 반복 횟수\n",
    "- **Bactch Size** - 각 epoch에서 모델에 보여지는 데이터 샘플 수 \n",
    "- **Learning Rate** - 각 batch와 epoch에서의 업데이트 크기로 작은 값는 학습 속도가 느려지지만, 큰 값은 학습 동안 예측할 수 없는 결과를 초래함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "demanding-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-dining",
   "metadata": {},
   "source": [
    "## Add an optimization loop\n",
    "\n",
    "하이퍼파라미터를 설정한 다음으로 최적화 반복을 사용하여 모델 학습과 최적화할 수 있음. 최적화를 위한 각 반복을 **epoch**라 부름. 각 epoch는 두 부분으로 구성됨.\n",
    "- The Train Loop - 데이터 전체를 학습하고 최적 파라미터로 수렴을 반복적으로 시도함\n",
    "- The validation/Test Loop - 테스트 데이터 전반에서 모델 성능이 향상되었는지 확인을 반복함\n",
    "\n",
    "### Add a loss function\n",
    "손실함수는 목표값과 모델로 부터 얻은 값 사이의 차이를 측정하므로 학습하는 동안 최소화되길 원함. 손실을 계산하기 위해 주어진 데이터 샘플을 사용하여 예측한 값과 실제  값의 차이를 비교함\n",
    "\n",
    "일반적인 손실 함수는 [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (Mean Square Error, regression), [nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) (Negative Log Likelihood, classification), [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) (combines `nn.LogSoftmax` and `nn.NLLLoss`.)\n",
    "\n",
    "출력 logit을 `nn.CrossEntropyLoss`로 전달하면, logit을 정규화하고 및 예측 에러를 계산함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "circular-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-height",
   "metadata": {},
   "source": [
    "### Optimization pass\n",
    "- 최적화는 각 학습 단계에서 모델 오차를 줄이기 위해 파라미터를 조절하는 단계\n",
    "- 모든 최적화 로직은 `optimizer` object로 캡슐레이션 되어있음\n",
    "- ADAM, RMSProp와 같은 다양한 최적화 알고리즘을 사용할 수 있음 ([different optimizers](https://pytorch.org/docs/stable/optim.html))\n",
    "- 학습률를 전달하고 학습에 필요한 파라미터를 등록함으로써 optimizer를 초기화 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "grave-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-standing",
   "metadata": {},
   "source": [
    "반복 학습과정 내부에는 최적화가 3단계로 진행됨\n",
    "- 모델 파라미터의 경사 초기화를 위해 `optimizer.zero_grad()` 호출함. 기본값에 의해 경사는 누적됨. 이중 계산을 막기 위해 각 반복에서 명시적으로 0을 초기화해야 함\n",
    "- `loss.backwards()`을 사용하여 예측 오차를 역전파. Pytorch는 각 파라미터 손실의 경사를 저장함.\n",
    "- `optimizer.step()` 호출하면 역전파 경로에서 수집된 경사로 인해 파라미터를 조절할 수 있음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-poland",
   "metadata": {},
   "source": [
    "### Full implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "subtle-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-alloy",
   "metadata": {},
   "source": [
    "`train_loop` 와 `test_loop`에 손실함수, 최적화 함수를 전달. 모델 성능향상을 위해 epoch 수를 높게 조절할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "different-volleyball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.300640  [    0/60000]\n",
      "loss: 2.290309  [ 6400/60000]\n",
      "loss: 2.286352  [12800/60000]\n",
      "loss: 2.284961  [19200/60000]\n",
      "loss: 2.274725  [25600/60000]\n",
      "loss: 2.273580  [32000/60000]\n",
      "loss: 2.268600  [38400/60000]\n",
      "loss: 2.267196  [44800/60000]\n",
      "loss: 2.237566  [51200/60000]\n",
      "loss: 2.240211  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 0.035035 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.225451  [    0/60000]\n",
      "loss: 2.211274  [ 6400/60000]\n",
      "loss: 2.202846  [12800/60000]\n",
      "loss: 2.202437  [19200/60000]\n",
      "loss: 2.185632  [25600/60000]\n",
      "loss: 2.186528  [32000/60000]\n",
      "loss: 2.178825  [38400/60000]\n",
      "loss: 2.179127  [44800/60000]\n",
      "loss: 2.110761  [51200/60000]\n",
      "loss: 2.115404  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.033081 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.113491  [    0/60000]\n",
      "loss: 2.087825  [ 6400/60000]\n",
      "loss: 2.064968  [12800/60000]\n",
      "loss: 2.056619  [19200/60000]\n",
      "loss: 2.046947  [25600/60000]\n",
      "loss: 2.065896  [32000/60000]\n",
      "loss: 2.034297  [38400/60000]\n",
      "loss: 2.052040  [44800/60000]\n",
      "loss: 1.928051  [51200/60000]\n",
      "loss: 1.933435  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 0.030317 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.959605  [    0/60000]\n",
      "loss: 1.917577  [ 6400/60000]\n",
      "loss: 1.876417  [12800/60000]\n",
      "loss: 1.854135  [19200/60000]\n",
      "loss: 1.876710  [25600/60000]\n",
      "loss: 1.927602  [32000/60000]\n",
      "loss: 1.856583  [38400/60000]\n",
      "loss: 1.912474  [44800/60000]\n",
      "loss: 1.733532  [51200/60000]\n",
      "loss: 1.732963  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 0.027457 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.802685  [    0/60000]\n",
      "loss: 1.753590  [ 6400/60000]\n",
      "loss: 1.699848  [12800/60000]\n",
      "loss: 1.670871  [19200/60000]\n",
      "loss: 1.725096  [25600/60000]\n",
      "loss: 1.806880  [32000/60000]\n",
      "loss: 1.705585  [38400/60000]\n",
      "loss: 1.798454  [44800/60000]\n",
      "loss: 1.583645  [51200/60000]\n",
      "loss: 1.581472  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 0.025258 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.672861  [    0/60000]\n",
      "loss: 1.625977  [ 6400/60000]\n",
      "loss: 1.560068  [12800/60000]\n",
      "loss: 1.539769  [19200/60000]\n",
      "loss: 1.602993  [25600/60000]\n",
      "loss: 1.703898  [32000/60000]\n",
      "loss: 1.589475  [38400/60000]\n",
      "loss: 1.705483  [44800/60000]\n",
      "loss: 1.474349  [51200/60000]\n",
      "loss: 1.473977  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.023585 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.566365  [    0/60000]\n",
      "loss: 1.526280  [ 6400/60000]\n",
      "loss: 1.445409  [12800/60000]\n",
      "loss: 1.441497  [19200/60000]\n",
      "loss: 1.504358  [25600/60000]\n",
      "loss: 1.614698  [32000/60000]\n",
      "loss: 1.499078  [38400/60000]\n",
      "loss: 1.628716  [44800/60000]\n",
      "loss: 1.391450  [51200/60000]\n",
      "loss: 1.392760  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.022282 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.478979  [    0/60000]\n",
      "loss: 1.448136  [ 6400/60000]\n",
      "loss: 1.352083  [12800/60000]\n",
      "loss: 1.363494  [19200/60000]\n",
      "loss: 1.427865  [25600/60000]\n",
      "loss: 1.541563  [32000/60000]\n",
      "loss: 1.429166  [38400/60000]\n",
      "loss: 1.566872  [44800/60000]\n",
      "loss: 1.327564  [51200/60000]\n",
      "loss: 1.332324  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 0.021271 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.408407  [    0/60000]\n",
      "loss: 1.386885  [ 6400/60000]\n",
      "loss: 1.277360  [12800/60000]\n",
      "loss: 1.301832  [19200/60000]\n",
      "loss: 1.368592  [25600/60000]\n",
      "loss: 1.483817  [32000/60000]\n",
      "loss: 1.374836  [38400/60000]\n",
      "loss: 1.522137  [44800/60000]\n",
      "loss: 1.278219  [51200/60000]\n",
      "loss: 1.285619  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.020485 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.349491  [    0/60000]\n",
      "loss: 1.338654  [ 6400/60000]\n",
      "loss: 1.218173  [12800/60000]\n",
      "loss: 1.250558  [19200/60000]\n",
      "loss: 1.322092  [25600/60000]\n",
      "loss: 1.438170  [32000/60000]\n",
      "loss: 1.331827  [38400/60000]\n",
      "loss: 1.489216  [44800/60000]\n",
      "loss: 1.239552  [51200/60000]\n",
      "loss: 1.248883  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.019864 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-atmosphere",
   "metadata": {},
   "source": [
    "# Save, Load and run model predictions \n",
    "모델 상태를 지속하기 위해 저장, 읽은 후 예측 모델을 동작하는 방법을 다룸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "composite-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx as onnx\n",
    "import torchvision.models as models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-clock",
   "metadata": {},
   "source": [
    "## Saving and loading model weights\n",
    "pytorch 모델은 학습한 파라미터를 `state_dict`이라고 하는 내부 dictionary에 보관함. `torch.save` 함수를 사용하여 이를 저장할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "wireless-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "torch.save(model.state_dict(), \"data/model_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-applicant",
   "metadata": {},
   "source": [
    "모델의 가중치를 다시 읽기 위해, 구조가 같은 모델 인스턴스를 만들고 load_state_dict() method로 파라미터를 읽어서 적용할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "exotic-recruitment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16()\n",
    "model.load_state_dict(torch.load(\"data/model_weights.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-violence",
   "metadata": {},
   "source": [
    "> **Note:** dropout이나 batch norm이 평가모드로 설정되도록 `model.eval()` 함수를 평가 전 호출했는지 확인해야 함. 호출하지 않았다면 일관적이지 않은 결과를 도출할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-request",
   "metadata": {},
   "source": [
    "## Saving and loading models with shapes\n",
    "모델의 가중치를 읽을 때에는 모델 클래스에 신경망의 구조가 정의되어 있으므로 모델 클래스의 인스턴스를 먼저 만드는 것이 필요함. 때문에 신경망의 파라미터와 더불어 신경망의 구조를 함께 저장하기 원한다면 `model.state_dict()` 대신 `model` 자체를 저장하거나 읽을 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fifty-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'data/vgg_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aboriginal-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('data/vgg_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-melissa",
   "metadata": {},
   "source": [
    "## Exporting the model to ONNX\n",
    "- 다른 플랫폼, 다른 언어에서도 테스트가 가능하도록 기능 지원(ONNX runtime 필요)\n",
    "- input_image는 맞은 자료형과 모양이라면 랜덤하게 결정되어도 무방함. sample data 개념.\n",
    "\n",
    "[참고자료]\n",
    "- [Pytorch를 모델을 ONNX으로 변환하고 ONNX 런타임에서 실행하기](https://tutorials.pytorch.kr/advanced/super_resolution_with_onnxruntime.html)\n",
    "- [Pytorch를 ONNX에서 export 하기](https://yunmorning.tistory.com/17)\n",
    "- https://netron.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "suffering-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.zeros((1,3,224,224))\n",
    "onnx.export(model, input_image, 'data/model.onnx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
